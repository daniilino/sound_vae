{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77 of 200\r"
     ]
    }
   ],
   "source": [
    "import soundcard as sc\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "SAMPLE_RATE = 48000 # [Hz]. sampling rate.\n",
    "\n",
    "duration = 200  # Duration of the audio stream in seconds\n",
    "window_size = 1024  # Size of the FFT window\n",
    "overlap = 512  # Number of samples to overlap between consecutive windows\n",
    "\n",
    "with sc.get_microphone(id=str(sc.default_speaker().name), include_loopback=True).recorder(samplerate=SAMPLE_RATE) as mic:\n",
    "    for i in range(duration):\n",
    "        data = mic.record(numframes=window_size)\n",
    "        data = torch.from_numpy(data).permute(1, 0)\n",
    "\n",
    "        print(f\"{i} of {duration}\", end=\"\\r\")\n",
    "\n",
    "        audio_spectogram = torchaudio.transforms.Spectrogram()(data)\n",
    "        audio_spectogram = audio_spectogram.log2()[0,:,:].numpy()\n",
    "        audio_spectogram -= audio_spectogram.min()\n",
    "        audio_spectogram /= audio_spectogram.max()\n",
    "\n",
    "        cv2.imshow('stream', audio_spectogram)\n",
    "\n",
    "        k = cv2.waitKey(33)\n",
    "        if k==27:    # Esc key to stop\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "        # # closing all open windows\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa \n",
    "import librosa.display\n",
    "\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_path = \"space3.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(sound_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound, sr = librosa.load(sound_path, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound duration:\n",
    "duration = len(sound) / sr \n",
    "print(f\"duration: {duration:3.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform visualization\n",
    "print(\"yo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### converting form mp3 to wav\n",
    "\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# files                                                                         \n",
    "src = r\"C:\\Users\\Admin\\Downloads\\MediaHuman\\Music\\Thip Trong - Lightvessel.mp3\"\n",
    "dst = \"test.wav\"\n",
    "\n",
    "# convert wav to mp3                                                            \n",
    "to_wav = AudioSegment.from_mp3(src)\n",
    "to_wav.export(dst, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_path = \"test.wav\"\n",
    "sound, sr = librosa.load(sound_path, mono=True)\n",
    "\n",
    "sound = sound[:90000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True)\n",
    "ax.set_ylim((-1.2, 1.2))\n",
    "librosa.display.waveshow(sound, sr=sr, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 1024\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "def amplitude_envelope(signal, frame_size, hop_length):\n",
    "    amplitude_envelope = []\n",
    "\n",
    "    for i in range(0, len(signal), hop_length):\n",
    "        current_frame_ae = max(signal[i:i+frame_size]) \n",
    "        amplitude_envelope.append(current_frame_ae)\n",
    "\n",
    "    return np.array(amplitude_envelope)\n",
    "\n",
    "def amplitude_envelope_np(signal, frame_size, hop_length):\n",
    "    return np.array([np.max(signal[i:i+frame_size]) for i in range(0, len(signal), hop_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_test = amplitude_envelope_np(sound, FRAME_SIZE, HOP_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ae_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(0, ae_test.size)\n",
    "t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True)\n",
    "ax.set_ylim((-1.2, 1.2))\n",
    "librosa.display.waveshow(sound, sr=sr, ax=ax)\n",
    "plt.plot(t, ae_test, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_test = librosa.feature.rms(y=sound, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
    "\n",
    "def my_rms(signal, frame_size, hop_lenegth):\n",
    "    ae = []\n",
    "    for i in range(0, len(signal), hop_lenegth):\n",
    "        current = signal[i:i+frame_size]\n",
    "        current = np.sqrt(np.mean(current**2))\n",
    "        ae.append(current)\n",
    "\n",
    "    return np.array(ae)\n",
    "\n",
    "rms_test_my = my_rms(sound, FRAME_SIZE, HOP_LENGTH)\n",
    "\n",
    "print(rms_test.shape, rms_test_my.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(0, rms_test.size)\n",
    "t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True)\n",
    "ax.set_ylim((-1.2, 1.2))\n",
    "librosa.display.waveshow(sound, sr=sr, ax=ax)\n",
    "plt.plot(t, rms_test, color='r')\n",
    "plt.plot(t, rms_test_my, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zcr_test = librosa.feature.zero_crossing_rate(y=sound, frame_length=FRAME_SIZE, hop_length=HOP_LENGTH)[0]\n",
    "zcr_test_my = [ np.sum(np.abs(np.diff(sound[i:i+FRAME_SIZE] > 0)) > 0) / FRAME_SIZE   for i in range(0, len(sound), HOP_LENGTH)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zcr_test[1:21])\n",
    "print(zcr_test_my[:20])\n",
    "\n",
    "print((np.array(zcr_test[1:101]) / np.array(zcr_test_my[:100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(0, rms_test.size)\n",
    "t = librosa.frames_to_time(frames, hop_length=HOP_LENGTH)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True)\n",
    "ax.set_ylim((-1.2, 1.2))\n",
    "librosa.display.waveshow(sound, sr=sr, ax=ax)\n",
    "plt.plot(t, rms_test, color='r')\n",
    "plt.plot(t, zcr_test, color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeAudioStream:\n",
    "    def __init__(self, sample_rate = 44100, window_size = 1024, overlap = 512, buffer_seconds = 5, cv2_window_size = (256, 512)):\n",
    "        \n",
    "        self.cv2_window_size = cv2_window_size # (H, W)\n",
    "\n",
    "        self.done = None\n",
    "        self.current_rms = None\n",
    "        self.current_zcr = None\n",
    "\n",
    "        self._d =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.sample_rate = (sample_rate // window_size) * window_size # samples per seconds, a.k.a [Hz]\n",
    "        print(f\"RealTimeAudioStream initialized with {self.sample_rate} sample rate\")\n",
    "        self.window_size = window_size # samples per processsing step\n",
    "        self.overlap = overlap # overlap\n",
    "\n",
    "        self._mic = sc.get_microphone(id=str(sc.default_speaker().name), include_loopback=True)\n",
    "        self._num_channels = self._mic.channels\n",
    "\n",
    "        self._buffer_size = self.sample_rate * buffer_seconds # samples memory size\n",
    "        self._buffer_wav = torch.zeros((self._buffer_size, self._num_channels), dtype=float, device=self._d)\n",
    "\n",
    "        self.buffer_rms = torch.zeros((self._buffer_size // overlap, self._num_channels), dtype=float, device=self._d)\n",
    "        self.buffer_zcr = torch.zeros((self._buffer_size // overlap, self._num_channels), dtype=float, device=self._d)\n",
    "\n",
    "    def _rms(self):\n",
    "        current = self._buffer_wav[-self.window_size:, :]\n",
    "        self.current_rms = current.pow(2).mean(0, keepdim=True).sqrt()\n",
    "        self.buffer_rms = torch.cat((self.buffer_rms, self.current_rms), dim=0)[1:,:]\n",
    "\n",
    "    def _zcr(self):\n",
    "        current = self._buffer_wav[-self.window_size:, :]\n",
    "        self.current_zcr = (torch.diff(current > 0, dim=0).type(torch.int).abs() > 0).sum(dim=0, keepdim=True) / self.window_size\n",
    "        self.buffer_zcr = torch.cat((self.buffer_zcr, self.current_zcr), dim=0)[1:,:]\n",
    "\n",
    "    def _vis(self):\n",
    "\n",
    "        show_R = self.buffer_rms\n",
    "        show_G = self.buffer_zcr\n",
    "\n",
    "        W1, C = show_R.shape\n",
    "\n",
    "        H, W = self.cv2_window_size\n",
    "        sound_R = torch.clamp(  H - ((show_R + 1) * H // 2)    , 0, H-1).type(torch.LongTensor)[:,0] # [W1]\n",
    "        sound_G = torch.clamp(  H - ((show_G + 1) * H // 2)    , 0, H-1).type(torch.LongTensor)[:,0] # [W1]\n",
    "        image = torch.zeros((H, W1, 3), dtype=float) # [H, W1]\n",
    "\n",
    "        image[sound_R, torch.arange(0, W1), 2] = 1\n",
    "        image[sound_G, torch.arange(0, W1), 1] = 1\n",
    "\n",
    "        image = cv2.resize(image.cpu().numpy(), (W, H))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def step(self, mic):\n",
    "        self._current = torch.from_numpy(mic.record(numframes=self.overlap)).to(self._d) # [window_size, num_channels] ~ [1024, 2]\n",
    "\n",
    "        self._buffer_wav = torch.cat((self._buffer_wav, self._current), dim=0)[self.overlap:,:]\n",
    "        self._rms()\n",
    "        self._zcr()\n",
    "\n",
    "        keyboard.on_press_key(\"ESC\", lambda _: self._done())\n",
    "\n",
    "        return self.current_rms, self.current_zcr\n",
    "    \n",
    "    def get_recorder(self):\n",
    "        return self._mic.recorder(samplerate=self.sample_rate)\n",
    "    \n",
    "    def _done(self):\n",
    "        self.done = True\n",
    "\n",
    "    def stream(self):\n",
    "        self.done = False\n",
    "\n",
    "        with self.get_recorder() as mic:\n",
    "            while not self.done:\n",
    "                self.step(mic)\n",
    "                cv2.imshow('stream', self._vis())\n",
    "\n",
    "                k = cv2.waitKey(33)\n",
    "                if k==27:    # Esc key to stop\n",
    "                    self.done = True\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n",
    "\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n"
     ]
    }
   ],
   "source": [
    "audio_stream = RealTimeAudioStream()\n",
    "\n",
    "with audio_stream.get_recorder() as mic:\n",
    "    while not audio_stream.done:\n",
    "        out = audio_stream.step(mic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_stream._buffer_wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = range(0, audio_stream.buffer_rms.shape[0])\n",
    "t = librosa.frames_to_time(frames, hop_length=audio_stream.overlap//2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True, sharey=True)\n",
    "ax.set_ylim((-1.2, 1.2))\n",
    "librosa.display.waveshow(audio_stream._buffer_wav[:,0], sr=audio_stream.sample_rate, ax=ax)\n",
    "plt.plot(t, audio_stream.buffer_rms[:,0], color='r')\n",
    "plt.plot(t, audio_stream.buffer_zcr[:,0], color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
