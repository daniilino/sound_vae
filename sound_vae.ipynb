{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundcard as sc\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import utils, datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.audio_utils import *\n",
    "from utils.vaes import *    \n",
    "from utils.data_utils import calculate_mean_std, split_train_val_test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torchaudio\\functional\\functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (256) may be set too high. Or, the value for `n_freqs` (513) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# stream your audio output and check what it visualizes\n",
    "\n",
    "audio_stream = RealTimeAudioStream()\n",
    "audio_stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix(dim):\n",
    "    # Generate a random orthogonal matrix\n",
    "    x = torch.randn(dim, dim).requires_grad_(False)\n",
    "    q, _ = torch.linalg.qr(x)\n",
    "    \n",
    "    # Apply the SVD decomposition to obtain the rotation matrix\n",
    "    u, _, v = torch.linalg.svd(q)\n",
    "    rotation_matrix = u @ v.mT\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def interpolate_transforms(matrix1, matrix2, steps):\n",
    "    # Interpolate between two rotation matrices\n",
    "    transforms = []\n",
    "    for t in range(steps + 1):\n",
    "        weight = t / steps\n",
    "        interpolated_matrix = torch.lerp(matrix1, matrix2, weight).requires_grad_(False)\n",
    "        transforms.append(interpolated_matrix)\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n"
     ]
    }
   ],
   "source": [
    "z_dim = 4\n",
    "mean =  [452.15845655661803, 70.35665321114695, 42.851305103906, 22.160291749979983]\n",
    "std1 =  [510.373306840422, 135.12631741685448, 102.5477751543778, 63.582527648859525]\n",
    "mean, std1 = torch.Tensor(mean).to(\"cuda\"), torch.Tensor(std1).to(\"cuda\")\n",
    "\n",
    "audio_stream = RealTimeAudioStream(z_dim=z_dim)\n",
    "vae = VAE(sample_x=example_data, hidden_dims=None, z_dim=z_dim)\n",
    "vae.load_state_dict(torch.load(r\"C:\\Users\\dan\\Desktop\\sound_vae\\models\\10_syntensor_only\\vae_43.pth\"))\n",
    "vae.to(\"cuda\")\n",
    "vae.eval()\n",
    "\n",
    "steps = 0 #current transformation matrix\n",
    "mat2 = random_rotation_matrix(z_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with audio_stream.get_recorder() as mic:\n",
    "        while not audio_stream.done:\n",
    "\n",
    "            rms, zcr, fft = audio_stream.step(mic)\n",
    "            rms, zcr = rms.mean().item(), zcr.mean().item()\n",
    "            \n",
    "            if steps==0:\n",
    "                steps = random.randint(2, 200)\n",
    "                mat1 = mat2.clone()\n",
    "                mat2 = random_rotation_matrix(z_dim)\n",
    "\n",
    "                rots = interpolate_transforms(mat2, mat1, steps)\n",
    "            \n",
    "            steps -= 1\n",
    "            r_m = rots[steps].to(\"cuda\")\n",
    "\n",
    "            # rms = ((rms - 0.15) * 12) # 0.3\n",
    "            # zcr = ((zcr - 0.07) * 24)# 0.14\n",
    "            # print(angle_r, rms, zcr, end=\"\\r\")\n",
    "            # z = (torch.tensor([rms, zcr]) @ r_m).unsqueeze(0).to(\"cuda\") \n",
    "            ######\n",
    "            z = fft[0, :, 0].float()\n",
    "            z = z - mean\n",
    "            z /= std1*30\n",
    "            z += 1\n",
    "            z = z @ r_m\n",
    "            ######\n",
    "            sample = vae.decoder(z)\n",
    "\n",
    "            image = sample[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "            image = cv2.resize(image, (2048, 1024))\n",
    "\n",
    "            cv2.imshow(\"generation\", image)\n",
    "\n",
    "            k = cv2.waitKey(33)\n",
    "            if k==27:    # Esc key to stop\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#audio_stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "batch_size_train = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=T.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=T.ToTensor(), download=False)\n",
    "\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "WIDTH = 128\n",
    "HEIGHT = 64\n",
    "\n",
    "dataset_dir = r\"data_synthetic\\10_syntensor_only\"\n",
    "\n",
    "# calculate dataset mean and std\n",
    "raw_transforms = T.Compose([\n",
    "    T.Resize((HEIGHT, WIDTH)), \n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "raw_data = datasets.ImageFolder(root = dataset_dir, transform = raw_transforms)\n",
    "\n",
    "# DATA_MEAN, DATA_STD = calculate_mean_std(raw_data)\n",
    "DATA_MEAN = [0.9386, 0.9386, 0.9386]\n",
    "DATA_STD  = [0.1825, 0.1825, 0.1825]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 64, 128])\n",
      "tensor(-5.1425)\n",
      "tensor(0.3364)\n"
     ]
    }
   ],
   "source": [
    "data_transforms = T.Compose([\n",
    "    T.Resize((HEIGHT, WIDTH)), \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=DATA_MEAN,std=DATA_STD),\n",
    "    T.Grayscale(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root = dataset_dir, transform = data_transforms)\n",
    "\n",
    "# split into Train, Val and Test\n",
    "data = split_train_val_test(dataset, val=0.0, test=0.1, batch_size=batch_size)\n",
    "\n",
    "examples = enumerate(data['train'])\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "print(example_data.min())\n",
    "print(example_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = []\n",
    "        labels = []\n",
    "        for x, y in test_loader:\n",
    "            mu, log_var = model.encoder(x.cuda())\n",
    "            z = model.sampling(mu, log_var).cpu().numpy()\n",
    "\n",
    "            latents.append(z)\n",
    "            labels.append(y)\n",
    "\n",
    "    latents = np.concatenate(latents, 0)\n",
    "    labels = np.concatenate(labels, 0)\n",
    "    model.train()\n",
    "\n",
    "    return latents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(model, loss_items, experiment_name, test_loader, z_dims):\n",
    "\n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "\n",
    "    extent = 5\n",
    "\n",
    "    cmap = plt.cm.tab20\n",
    "    bounds = np.linspace(0,10,11)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    latents, labels = eval_on_test(model, test_loader)\n",
    "    if extent is not None: \n",
    "        ax.set_xlim(-extent, extent)\n",
    "        ax.set_ylim(-extent, extent)\n",
    "    scat = ax.scatter(latents[:, 0], latents[:,1], s=2, marker='o', cmap=cmap, c=labels)\n",
    "    cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "\n",
    "    title = f\"Recon: {loss_items[0].item():2.3f}, KLD {loss_items[1].item():2.3f}\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    path1 = rf'latent_space_vis\\{experiment_name}'\n",
    "\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "\n",
    "    fig.savefig(path1 + rf'\\{pic_name}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return reconstruction error + KL divergence losses\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    B, C, H, W = recon_x.shape\n",
    "    beta = 0.01 #legend says, that the bigger beta is, the higher the disentanglement\n",
    "    recons_loss = F.mse_loss(recon_x.view(B, -1), x.view(B, -1), reduction=\"mean\")\n",
    "    KLD = beta * -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()) # 1 + log(sigma**2) - mu**2 - sigma**2\n",
    "    return recons_loss, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_image(image):\n",
    "    dtype = image.dtype\n",
    "    image = image.astype(float)\n",
    "    image = image - np.min(image)\n",
    "    image = image / np.max(image) * 255\n",
    "    image = image.astype(dtype)\n",
    "    return image\n",
    "\n",
    "def save_recon(x_recon, experiment_name):\n",
    "    image = x_recon[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    path =  rf\"latent_space_vis\\{experiment_name}\\recons\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{pic_name}.jpg\"), norm_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_f, train_loader, test_loader, optimizer, scheduler, epoch, experiment_name, embedding_size):\n",
    "    \n",
    "    if embedding_size > 2: vis = False\n",
    "    else: vis = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        std3 = log_var.exp().sqrt().mean()*3\n",
    "        x_recon = x_recon[:, 0, None, :, :]\n",
    "\n",
    "        rec, KLD = loss_f(x_recon, x, mu, log_var)\n",
    "        loss = rec + KLD\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(scheduler.get_last_lr())\n",
    "\n",
    "        if batch_idx % 25 == 0:\n",
    "            if vis: \n",
    "                visualize_latent_space(model, (rec, KLD), experiment_name, test_loader)\n",
    "                \n",
    "            save_recon(x_recon, experiment_name)\n",
    "            print(\"Epoch {:3} Iteration {:3}: recon: {:8.4f}, kld: {:8.4f}, std3: {:2.4f}\".format(epoch, batch_idx, rec.item(), KLD.item(), std3.item()))\n",
    "\n",
    "    path =  rf\"models\\{experiment_name}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    save_model_to = rf\"{path}\\vae_{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), save_model_to)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "\n",
    "\n",
    "# build model\n",
    "vae = VAE(sample_x=example_data, hidden_dims=None, z_dim=embedding_size)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### sanity check\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vae.to(device)\n",
    "# vae.train()\n",
    "\n",
    "# for batch_idx, (x, _) in enumerate(data['train']):\n",
    "#     x = x.to(device)\n",
    "#     out, mu, log_var = vae(x)\n",
    "#     print(out.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Iteration   0: recon:   0.6028, kld:   0.0341, std3: 0.1054\n",
      "Epoch   2 Iteration   0: recon:   1.1233, kld:   0.0206, std3: 0.3361\n",
      "Epoch   3 Iteration   0: recon:   1.0040, kld:   0.0258, std3: 0.2344\n",
      "Epoch   4 Iteration   0: recon:   0.9204, kld:   0.0244, std3: 0.2638\n",
      "Epoch   5 Iteration   0: recon:   0.9266, kld:   0.0252, std3: 0.2394\n",
      "Epoch   6 Iteration   0: recon:   0.9280, kld:   0.0253, std3: 0.2362\n",
      "Epoch   7 Iteration   0: recon:   0.9121, kld:   0.0267, std3: 0.2050\n",
      "Epoch   8 Iteration   0: recon:   0.8291, kld:   0.0285, std3: 0.1768\n",
      "Epoch   9 Iteration   0: recon:   0.8084, kld:   0.0297, std3: 0.1615\n",
      "Epoch  10 Iteration   0: recon:   0.7269, kld:   0.0314, std3: 0.1378\n",
      "Epoch  11 Iteration   0: recon:   0.6539, kld:   0.0314, std3: 0.1450\n",
      "Epoch  12 Iteration   0: recon:   0.6075, kld:   0.0329, std3: 0.1227\n",
      "Epoch  13 Iteration   0: recon:   0.5927, kld:   0.0341, std3: 0.1101\n",
      "Epoch  14 Iteration   0: recon:   0.5680, kld:   0.0339, std3: 0.1110\n",
      "Epoch  15 Iteration   0: recon:   0.5908, kld:   0.0344, std3: 0.1063\n",
      "Epoch  16 Iteration   0: recon:   0.5608, kld:   0.0342, std3: 0.1065\n",
      "Epoch  17 Iteration   0: recon:   0.5722, kld:   0.0351, std3: 0.0986\n",
      "Epoch  18 Iteration   0: recon:   0.6383, kld:   0.0342, std3: 0.1052\n",
      "Epoch  19 Iteration   0: recon:   0.5752, kld:   0.0337, std3: 0.1104\n",
      "Epoch  20 Iteration   0: recon:   0.5194, kld:   0.0346, std3: 0.1019\n",
      "Epoch  21 Iteration   0: recon:   0.4850, kld:   0.0349, std3: 0.0992\n",
      "Epoch  22 Iteration   0: recon:   0.4710, kld:   0.0354, std3: 0.0955\n",
      "Epoch  23 Iteration   0: recon:   0.4793, kld:   0.0357, std3: 0.0917\n",
      "Epoch  24 Iteration   0: recon:   0.4704, kld:   0.0360, std3: 0.0888\n",
      "Epoch  25 Iteration   0: recon:   0.4517, kld:   0.0358, std3: 0.0897\n",
      "Epoch  26 Iteration   0: recon:   0.4642, kld:   0.0357, std3: 0.0905\n",
      "Epoch  27 Iteration   0: recon:   0.4732, kld:   0.0363, std3: 0.0874\n",
      "Epoch  28 Iteration   0: recon:   0.6022, kld:   0.0350, std3: 0.0949\n",
      "Epoch  29 Iteration   0: recon:   0.4772, kld:   0.0357, std3: 0.0893\n",
      "Epoch  30 Iteration   0: recon:   0.4338, kld:   0.0357, std3: 0.0880\n",
      "Epoch  31 Iteration   0: recon:   0.4272, kld:   0.0363, std3: 0.0852\n",
      "Epoch  32 Iteration   0: recon:   0.4242, kld:   0.0364, std3: 0.0852\n",
      "Epoch  33 Iteration   0: recon:   0.4239, kld:   0.0366, std3: 0.0824\n",
      "Epoch  34 Iteration   0: recon:   0.4160, kld:   0.0365, std3: 0.0836\n",
      "Epoch  35 Iteration   0: recon:   0.4079, kld:   0.0372, std3: 0.0762\n",
      "Epoch  36 Iteration   0: recon:   0.4188, kld:   0.0365, std3: 0.0828\n",
      "Epoch  37 Iteration   0: recon:   0.5258, kld:   0.0366, std3: 0.0848\n",
      "Epoch  38 Iteration   0: recon:   0.4216, kld:   0.0375, std3: 0.0768\n",
      "Epoch  39 Iteration   0: recon:   0.4152, kld:   0.0368, std3: 0.0829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m experiment_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m10_syntensor_only\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m---> 11\u001b[0m     train(vae, vae_loss, data[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], data[\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m], optimizer, scheduler, epoch, experiment_name, embedding_size\u001b[39m=\u001b[39;49membedding_size)\n",
      "Cell \u001b[1;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loss_f, train_loader, test_loader, optimizer, scheduler, epoch, experiment_name, embedding_size)\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m x_recon, mu, log_var \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m     15\u001b[0m std3 \u001b[39m=\u001b[39m log_var\u001b[39m.\u001b[39mexp()\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39mmean()\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[0;32m     16\u001b[0m x_recon \u001b[39m=\u001b[39m x_recon[:, \u001b[39m0\u001b[39m, \u001b[39mNone\u001b[39;00m, :, :]\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dan\\Desktop\\sound_vae\\utils\\vaes.py:137\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 137\u001b[0m     mu, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m    138\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msampling(mu, log_var)\n\u001b[0;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z), mu, log_var\n",
      "File \u001b[1;32mc:\\Users\\dan\\Desktop\\sound_vae\\utils\\vaes.py:118\u001b[0m, in \u001b[0;36mVAE.encoder\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencoder\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 118\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_layers(x)\n\u001b[0;32m    119\u001b[0m     h \u001b[39m=\u001b[39m h\u001b[39m.\u001b[39mreshape(h\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_mu(h), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_var(h)\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# remember: acceptable recon loss is 0.45\n",
    "num_epochs = 200\n",
    "\n",
    "experiment_name = f\"10_syntensor_only\"\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(vae, vae_loss, data['train'], data['test'], optimizer, scheduler, epoch, experiment_name, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'utils.vaes.VAE'>: it's not the same object as utils.vaes.VAE",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[85], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49msave(vae, \u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mvae_orbita.pth\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    440\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dan\\anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\serialization.py:653\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    651\u001b[0m pickler \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mPickler(data_buf, protocol\u001b[39m=\u001b[39mpickle_protocol)\n\u001b[0;32m    652\u001b[0m pickler\u001b[39m.\u001b[39mpersistent_id \u001b[39m=\u001b[39m persistent_id\n\u001b[1;32m--> 653\u001b[0m pickler\u001b[39m.\u001b[39;49mdump(obj)\n\u001b[0;32m    654\u001b[0m data_value \u001b[39m=\u001b[39m data_buf\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    655\u001b[0m zip_file\u001b[39m.\u001b[39mwrite_record(\u001b[39m'\u001b[39m\u001b[39mdata.pkl\u001b[39m\u001b[39m'\u001b[39m, data_value, \u001b[39mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class 'utils.vaes.VAE'>: it's not the same object as utils.vaes.VAE"
     ]
    }
   ],
   "source": [
    "torch.save(vae, r\"models\\vae_orbita.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
