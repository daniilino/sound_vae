{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundcard as sc\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import utils, datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealTimeAudioStream:\n",
    "    def __init__(self, sample_rate = 44100, window_size = 1024, overlap = 512, buffer_seconds = 5, cv2_window_size = (256, 512)):\n",
    "        \n",
    "        self.cv2_window_size = cv2_window_size # (H, W)\n",
    "\n",
    "        self.done = None\n",
    "        self.current_rms = None\n",
    "        self.current_zcr = None\n",
    "\n",
    "        self._d =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.sample_rate = (sample_rate // window_size) * window_size # samples per seconds, a.k.a [Hz]\n",
    "        print(f\"RealTimeAudioStream initialized with {self.sample_rate} sample rate\")\n",
    "        self.window_size = window_size # samples per processsing step\n",
    "        self.overlap = overlap # overlap\n",
    "\n",
    "        self._mic = sc.get_microphone(id=str(sc.default_speaker().name), include_loopback=True)\n",
    "        self._num_channels = self._mic.channels\n",
    "\n",
    "        self._buffer_size = self.sample_rate * buffer_seconds # samples memory size\n",
    "        self._buffer_wav = torch.zeros((self._buffer_size, self._num_channels), dtype=float, device=self._d)\n",
    "\n",
    "        self.buffer_rms = torch.zeros((self._buffer_size // overlap, self._num_channels), dtype=float, device=self._d)\n",
    "        self.buffer_zcr = torch.zeros((self._buffer_size // overlap, self._num_channels), dtype=float, device=self._d)\n",
    "\n",
    "    def _rms(self):\n",
    "        current = self._buffer_wav[-self.window_size:, :]\n",
    "        self.current_rms = current.pow(2).mean(0, keepdim=True).sqrt()\n",
    "        self.buffer_rms = torch.cat((self.buffer_rms, self.current_rms), dim=0)[1:,:]\n",
    "\n",
    "    def _zcr(self):\n",
    "        current = self._buffer_wav[-self.window_size:, :]\n",
    "        self.current_zcr = (torch.diff(current > 0, dim=0).type(torch.int).abs() > 0).sum(dim=0, keepdim=True) / self.window_size\n",
    "        self.buffer_zcr = torch.cat((self.buffer_zcr, self.current_zcr), dim=0)[1:,:]\n",
    "\n",
    "    def _vis(self):\n",
    "\n",
    "        show_R = self.buffer_rms\n",
    "        show_G = self.buffer_zcr\n",
    "\n",
    "        W1, C = show_R.shape\n",
    "\n",
    "        H, W = self.cv2_window_size\n",
    "        sound_R = torch.clamp(  H - ((show_R + 1) * H // 2)    , 0, H-1).type(torch.LongTensor)[:,0] # [W1]\n",
    "        sound_G = torch.clamp(  H - ((show_G + 1) * H // 2)    , 0, H-1).type(torch.LongTensor)[:,0] # [W1]\n",
    "        image = torch.zeros((H, W1, 3), dtype=float) # [H, W1]\n",
    "\n",
    "        image[sound_R, torch.arange(0, W1), 2] = 1\n",
    "        image[sound_G, torch.arange(0, W1), 1] = 1\n",
    "\n",
    "        image = cv2.resize(image.cpu().numpy(), (W, H))\n",
    "\n",
    "        return image\n",
    "\n",
    "    def step(self, mic):\n",
    "        self._current = torch.from_numpy(mic.record(numframes=self.overlap)).to(self._d) # [window_size, num_channels] ~ [1024, 2]\n",
    "\n",
    "        self._buffer_wav = torch.cat((self._buffer_wav, self._current), dim=0)[self.overlap:,:]\n",
    "        self._rms()\n",
    "        self._zcr()\n",
    "\n",
    "        keyboard.on_press_key(\"ESC\", lambda _: self._done())\n",
    "\n",
    "        return self.current_rms, self.current_zcr\n",
    "    \n",
    "    def get_recorder(self):\n",
    "        return self._mic.recorder(samplerate=self.sample_rate)\n",
    "    \n",
    "    def _done(self):\n",
    "        self.done = True\n",
    "\n",
    "    def stream(self):\n",
    "        self.done = False\n",
    "\n",
    "        with self.get_recorder() as mic:\n",
    "            while not self.done:\n",
    "                self.step(mic)\n",
    "                cv2.imshow('stream', self._vis())\n",
    "\n",
    "                k = cv2.waitKey(33)\n",
    "                if k==27:    # Esc key to stop\n",
    "                    self.done = True\n",
    "                    cv2.destroyAllWindows()\n",
    "                    break\n",
    "\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n",
      "28.97246558310587 1.6812304722883726 3.371874999999999702773\r"
     ]
    }
   ],
   "source": [
    "audio_stream = RealTimeAudioStream()\n",
    "vae = torch.load(r\"models\\vae_mnist_2dim.pth\")\n",
    "vae.eval()\n",
    "\n",
    "angle = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    with audio_stream.get_recorder() as mic:\n",
    "        while not audio_stream.done:\n",
    "            angle += 2\n",
    "            angle_r = math.radians(angle)\n",
    "            \n",
    "            r_m = torch.tensor([[math.cos(angle_r), -math.sin(angle_r)], \n",
    "                                [math.sin(angle_r), math.cos(angle_r),]])\n",
    "\n",
    "            rms, zcr = audio_stream.step(mic)\n",
    "\n",
    "            rms = (rms.mean().item() - 0.15) * 20 # 0.3\n",
    "            zcr = (zcr.mean().item() - 0.07) * 40# 0.14\n",
    "            print(angle_r, rms, zcr, end=\"\\r\")\n",
    "            z = (torch.tensor([rms, zcr]) @ r_m).unsqueeze(0).to(\"cuda\") \n",
    "            sample = vae.decoder(z)\n",
    "\n",
    "            image = sample[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "            image = cv2.resize(image, (256, 256))\n",
    "\n",
    "            cv2.imshow(\"generation\", image)\n",
    "\n",
    "            k = cv2.waitKey(33)\n",
    "            if k==27:    # Esc key to stop\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#audio_stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 1, 28, 28])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_train = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=T.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=T.ToTensor(), download=False)\n",
    "\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, sample_x, hidden_dims, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            self.hidden_dims = [2, 4, 8]\n",
    "\n",
    "        # Build Encoder\n",
    "        in_channels, out_size, out_size = sample_x.shape\n",
    "        K, S, P = 3, 2, 1\n",
    "        for h_dim in self.hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size = K, stride= S, padding = P),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            out_size = int(((out_size - K + (2*P)) / S) + 1)\n",
    "\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder_layers = nn.Sequential(*modules)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.hidden_dims[-1]*out_size*out_size, z_dim) # [B, Z_dim]\n",
    "        self.fc_var = nn.Linear(self.hidden_dims[-1]*out_size*out_size, z_dim) # [B, Z_dim]\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(z_dim, self.hidden_dims[-1] * out_size * out_size)\n",
    "\n",
    "        self.hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(self.hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(self.hidden_dims[i],\n",
    "                                       self.hidden_dims[i + 1],\n",
    "                                       kernel_size=K,\n",
    "                                       stride = S,\n",
    "                                       padding = P,\n",
    "                                       output_padding = 1),\n",
    "                    nn.BatchNorm2d(self.hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder_layers = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(self.hidden_dims[-1],\n",
    "                                               self.hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(self.hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(self.hidden_dims[-1], out_channels=1,\n",
    "                                      kernel_size= 7, padding= 1),\n",
    "                            nn.Sigmoid())\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = self.encoder_layers(x)\n",
    "        h = h.reshape(h.shape[0], -1)\n",
    "        return self.fc_mu(h), self.fc_var(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample = g(x, eps)\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = self.decoder_input(z)\n",
    "        B, CHW = h.shape\n",
    "        C = self.hidden_dims[0]\n",
    "        HW = int((CHW // C)**0.5)\n",
    "        h = h.reshape(B, C, HW, HW)\n",
    "        h = self.decoder_layers(h)\n",
    "        h = self.final_layer(h)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = []\n",
    "        labels = []\n",
    "        for x, y in test_loader:\n",
    "            mu, log_var = model.encoder(x.cuda())\n",
    "            z = model.sampling(mu, log_var).cpu().numpy()\n",
    "\n",
    "            latents.append(z)\n",
    "            labels.append(y)\n",
    "\n",
    "    latents = np.concatenate(latents, 0)\n",
    "    labels = np.concatenate(labels, 0)\n",
    "    model.train()\n",
    "\n",
    "    return latents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(model, loss_items, experiment_name, test_loader):\n",
    "\n",
    "    latents, labels = eval_on_test(model, test_loader)\n",
    "    \n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "\n",
    "    extent = 5\n",
    "\n",
    "    cmap = plt.cm.tab20\n",
    "    bounds = np.linspace(0,10,11)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    if extent is not None: \n",
    "        ax.set_xlim(-extent, extent)\n",
    "        ax.set_ylim(-extent, extent)\n",
    "    scat = ax.scatter(latents[:, 0], latents[:,1], s=2, marker='o', cmap=cmap, c=labels)\n",
    "    cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "\n",
    "    title = f\"Recon: {loss_items[0].item():2.3f}, KLD {loss_items[1].item():2.3f}\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    path1 = rf'latent_space_vis\\{experiment_name}'\n",
    "\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "\n",
    "    fig.savefig(path1 + rf'\\{pic_name}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return reconstruction error + KL divergence losses\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    # recons_loss = F.binary_cross_entropy(recon_x.view(-1,784), x.view(-1, 784), reduction='mean')\n",
    "    recons_loss = F.mse_loss(recon_x.view(-1, 784), x.view(-1, 784), reduction=\"mean\") * 2000\n",
    "    KLD = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()) # 1 + log(sigma**2) - mu**2 - sigma**2\n",
    "    return recons_loss, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_image(image):\n",
    "    dtype = image.dtype\n",
    "    image = image.astype(float)\n",
    "    image = image - np.min(image)\n",
    "    image = image / np.max(image) * 255\n",
    "    image = image.astype(dtype)\n",
    "    return image\n",
    "\n",
    "def save_recon(x_recon, experiment_name):\n",
    "    image = x_recon[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    path =  rf\"latent_space_vis\\{experiment_name}\\recons\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{pic_name}.jpg\"), norm_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_f, train_loader, test_loader, optimizer, epoch, experiment_name):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, log_var = model(x)\n",
    "\n",
    "        rec, KLD = loss_f(x_recon, x, mu, log_var)\n",
    "        loss = rec + KLD\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 25 == 0:\n",
    "            if experiment_name is not None: \n",
    "                visualize_latent_space(model, (rec, KLD), experiment_name, test_loader)\n",
    "                save_recon(x_recon, experiment_name)\n",
    "                \n",
    "            print(\"Epoch {} Iteration {}: Loss = {}\".format(epoch, batch_idx, loss.item()))\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 2\n",
    "\n",
    "\n",
    "# build model\n",
    "vae = VAE(sample_x=example_data[0], hidden_dims=None, z_dim=2)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-2, weight_decay=0.99)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iteration 0: Loss = 390.8219909667969\n",
      "Epoch 1 Iteration 25: Loss = 131.3209686279297\n",
      "Epoch 1 Iteration 50: Loss = 121.09114837646484\n",
      "Epoch 1 Iteration 75: Loss = 119.03294372558594\n",
      "Epoch 1 Iteration 100: Loss = 116.9339599609375\n",
      "Epoch 1 Iteration 125: Loss = 113.74452209472656\n",
      "Epoch 1 Iteration 150: Loss = 114.39002990722656\n",
      "Epoch 1 Iteration 175: Loss = 112.44861602783203\n",
      "Epoch 1 Iteration 200: Loss = 113.15045928955078\n",
      "Epoch 1 Iteration 225: Loss = 113.28880310058594\n",
      "Epoch 2 Iteration 0: Loss = 112.61363220214844\n",
      "Epoch 2 Iteration 25: Loss = 108.91123962402344\n",
      "Epoch 2 Iteration 50: Loss = 110.76921844482422\n",
      "Epoch 2 Iteration 75: Loss = 112.32485961914062\n",
      "Epoch 2 Iteration 100: Loss = 105.51924896240234\n",
      "Epoch 2 Iteration 125: Loss = 108.69564056396484\n",
      "Epoch 2 Iteration 150: Loss = 107.24166870117188\n",
      "Epoch 2 Iteration 175: Loss = 101.22027587890625\n",
      "Epoch 2 Iteration 200: Loss = 109.8602523803711\n",
      "Epoch 2 Iteration 225: Loss = 113.53419494628906\n",
      "Epoch 3 Iteration 0: Loss = 106.36965942382812\n",
      "Epoch 3 Iteration 25: Loss = 105.81885528564453\n",
      "Epoch 3 Iteration 50: Loss = 106.85558319091797\n",
      "Epoch 3 Iteration 75: Loss = 101.2952651977539\n",
      "Epoch 3 Iteration 100: Loss = 103.94269561767578\n",
      "Epoch 3 Iteration 125: Loss = 102.14616394042969\n",
      "Epoch 3 Iteration 150: Loss = 104.46827697753906\n",
      "Epoch 3 Iteration 175: Loss = 104.51461029052734\n",
      "Epoch 3 Iteration 200: Loss = 105.42738342285156\n",
      "Epoch 3 Iteration 225: Loss = 106.79338836669922\n",
      "Epoch 4 Iteration 0: Loss = 104.10791015625\n",
      "Epoch 4 Iteration 25: Loss = 107.14534759521484\n",
      "Epoch 4 Iteration 50: Loss = 104.00588989257812\n",
      "Epoch 4 Iteration 75: Loss = 101.16383361816406\n",
      "Epoch 4 Iteration 100: Loss = 100.98006439208984\n",
      "Epoch 4 Iteration 125: Loss = 104.03074645996094\n",
      "Epoch 4 Iteration 150: Loss = 104.883544921875\n",
      "Epoch 4 Iteration 175: Loss = 100.16043090820312\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_820\\167499287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperiment_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_820\\3528514573.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, loss_f, train_loader, test_loader, optimizer, epoch, experiment_name)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrec\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mKLD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\Admin\\Anaconda3\\envs\\sound_vae\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "\n",
    "experiment_name = f\"1_rms_zcr\"\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(vae, vae_loss, train_loader, test_loader, optimizer, epoch, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae, r\"models\\vae_mnist_2dim.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
