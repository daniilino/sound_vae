{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundcard as sc\n",
    "import soundfile as sf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import keyboard\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision import utils, datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.audio_utils import *\n",
    "from utils.vaes import *    \n",
    "from utils.data_utils import calculate_mean_std, split_train_val_test\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n"
     ]
    }
   ],
   "source": [
    "# stream your audio output and check what it visualizes\n",
    "\n",
    "audio_stream = RealTimeAudioStream()\n",
    "audio_stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix(dim):\n",
    "    # Generate a random orthogonal matrix\n",
    "    x = torch.randn(dim, dim).requires_grad_(False)\n",
    "    q, _ = torch.linalg.qr(x)\n",
    "    \n",
    "    # Apply the SVD decomposition to obtain the rotation matrix\n",
    "    u, _, v = torch.linalg.svd(q)\n",
    "    rotation_matrix = u @ v.mT\n",
    "    \n",
    "    return rotation_matrix\n",
    "\n",
    "def interpolate_transforms(matrix1, matrix2, steps):\n",
    "    # Interpolate between two rotation matrices\n",
    "    transforms = []\n",
    "    for t in range(steps + 1):\n",
    "        weight = t / steps\n",
    "        interpolated_matrix = torch.lerp(matrix1, matrix2, weight).requires_grad_(False)\n",
    "        transforms.append(interpolated_matrix)\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RealTimeAudioStream initialized with 44032 sample rate\n"
     ]
    }
   ],
   "source": [
    "z_dim = 4\n",
    "mean =  [452.15845655661803, 70.35665321114695, 42.851305103906, 22.160291749979983]\n",
    "std1 =  [510.373306840422, 135.12631741685448, 102.5477751543778, 63.582527648859525]\n",
    "mean, std1 = torch.Tensor(mean).to(\"cuda\"), torch.Tensor(std1).to(\"cuda\")\n",
    "\n",
    "audio_stream = RealTimeAudioStream(z_dim=z_dim)\n",
    "vae = VAE(sample_x=example_data, hidden_dims=None, z_dim=z_dim)\n",
    "vae.load_state_dict(torch.load(r\"C:\\Users\\dan\\Desktop\\sound_vae\\models\\8_orbita_only.pth\"))\n",
    "vae.to(\"cuda\")\n",
    "vae.eval()\n",
    "\n",
    "steps = 0 #current transformation matrix\n",
    "mat2 = random_rotation_matrix(z_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    while not audio_stream.done:\n",
    "\n",
    "        rms, zcr, fft = audio_stream.step(None)\n",
    "        rms, zcr = rms.mean().item(), zcr.mean().item()\n",
    "        \n",
    "        if steps==0:\n",
    "            steps = random.randint(2, 200)\n",
    "            mat1 = mat2.clone()\n",
    "            mat2 = random_rotation_matrix(z_dim)\n",
    "\n",
    "            rots = interpolate_transforms(mat2, mat1, steps)\n",
    "        \n",
    "        steps -= 1\n",
    "        r_m = rots[steps].to(\"cuda\")\n",
    "\n",
    "        # rms = ((rms - 0.15) * 12) # 0.3\n",
    "        # zcr = ((zcr - 0.07) * 24)# 0.14\n",
    "        # print(angle_r, rms, zcr, end=\"\\r\")\n",
    "        # z = (torch.tensor([rms, zcr]) @ r_m).unsqueeze(0).to(\"cuda\") \n",
    "        ######\n",
    "        z = fft[0, :, 0].float()\n",
    "        z = z - mean\n",
    "        z /= std1*40\n",
    "        z += 0.5\n",
    "        z = z @ r_m\n",
    "        ######\n",
    "        sample = vae.decoder(z)\n",
    "\n",
    "        image = sample[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "        image = cv2.resize(image, (1024, 512))\n",
    "\n",
    "        cv2.imshow(\"generation\", image)\n",
    "\n",
    "        k = cv2.waitKey(33)\n",
    "        if k==27:    # Esc key to stop\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#audio_stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "\n",
    "batch_size_train = 256\n",
    "batch_size_test = 256\n",
    "\n",
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=T.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=T.ToTensor(), download=False)\n",
    "\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom dataset\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "WIDTH = 128\n",
    "HEIGHT = 64\n",
    "\n",
    "dataset_dir = r\"data_synthetic\\10_syntensor_only\"\n",
    "\n",
    "# calculate dataset mean and std\n",
    "raw_transforms = T.Compose([\n",
    "    T.Resize((HEIGHT, WIDTH)), \n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "raw_data = datasets.ImageFolder(root = dataset_dir, transform = raw_transforms)\n",
    "\n",
    "# DATA_MEAN, DATA_STD = calculate_mean_std(raw_data)\n",
    "DATA_MEAN = [0.9386, 0.9386, 0.9386]\n",
    "DATA_STD  = [0.1825, 0.1825, 0.1825]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 64, 128])\n",
      "tensor(-5.1425)\n",
      "tensor(0.3364)\n"
     ]
    }
   ],
   "source": [
    "data_transforms = T.Compose([\n",
    "    T.Resize((HEIGHT, WIDTH)), \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=DATA_MEAN,std=DATA_STD),\n",
    "    T.Grayscale(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root = dataset_dir, transform = data_transforms)\n",
    "\n",
    "# split into Train, Val and Test\n",
    "data = split_train_val_test(dataset, val=0.0, test=0.1, batch_size=batch_size)\n",
    "\n",
    "examples = enumerate(data['train'])\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "print(example_data.shape)\n",
    "print(example_data.min())\n",
    "print(example_data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latents = []\n",
    "        labels = []\n",
    "        for x, y in test_loader:\n",
    "            mu, log_var = model.encoder(x.cuda())\n",
    "            z = model.sampling(mu, log_var).cpu().numpy()\n",
    "\n",
    "            latents.append(z)\n",
    "            labels.append(y)\n",
    "\n",
    "    latents = np.concatenate(latents, 0)\n",
    "    labels = np.concatenate(labels, 0)\n",
    "    model.train()\n",
    "\n",
    "    return latents, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_latent_space(model, loss_items, experiment_name, test_loader, z_dims):\n",
    "\n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "\n",
    "    extent = 5\n",
    "\n",
    "    cmap = plt.cm.tab20\n",
    "    bounds = np.linspace(0,10,11)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    latents, labels = eval_on_test(model, test_loader)\n",
    "    if extent is not None: \n",
    "        ax.set_xlim(-extent, extent)\n",
    "        ax.set_ylim(-extent, extent)\n",
    "    scat = ax.scatter(latents[:, 0], latents[:,1], s=2, marker='o', cmap=cmap, c=labels)\n",
    "    cb = plt.colorbar(scat, spacing='proportional',ticks=bounds)\n",
    "\n",
    "    title = f\"Recon: {loss_items[0].item():2.3f}, KLD {loss_items[1].item():2.3f}\"\n",
    "    ax.set_title(title)\n",
    "\n",
    "    path1 = rf'latent_space_vis\\{experiment_name}'\n",
    "\n",
    "    if not os.path.exists(path1):\n",
    "        os.makedirs(path1)\n",
    "\n",
    "    fig.savefig(path1 + rf'\\{pic_name}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return reconstruction error + KL divergence losses\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    B, C, H, W = recon_x.shape\n",
    "    beta = 0.01 #legend says, that the bigger beta is, the higher the disentanglement\n",
    "    recons_loss = F.mse_loss(recon_x.view(B, -1), x.view(B, -1), reduction=\"mean\")\n",
    "    KLD = beta * -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp()) # 1 + log(sigma**2) - mu**2 - sigma**2\n",
    "    return recons_loss, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_image(image):\n",
    "    dtype = image.dtype\n",
    "    image = image.astype(float)\n",
    "    image = image - np.min(image)\n",
    "    image = image / np.max(image) * 255\n",
    "    image = image.astype(dtype)\n",
    "    return image\n",
    "\n",
    "def save_recon(x_recon, experiment_name):\n",
    "    image = x_recon[0].permute(1, 2, 0).detach().cpu().numpy()\n",
    "    now = datetime.now()\n",
    "    pic_name = now.strftime(\"%Y%m%d%H%M%S%f\")\n",
    "    path =  rf\"latent_space_vis\\{experiment_name}\\recons\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    cv2.imwrite(os.path.join(path, f\"{pic_name}.jpg\"), norm_image(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_f, train_loader, test_loader, optimizer, scheduler, epoch, experiment_name, embedding_size):\n",
    "    \n",
    "    if embedding_size > 2: vis = False\n",
    "    else: vis = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x_recon, mu, log_var = model(x)\n",
    "        std3 = log_var.exp().sqrt().mean()*3\n",
    "        x_recon = x_recon[:, 0, None, :, :]\n",
    "\n",
    "        rec, KLD = loss_f(x_recon, x, mu, log_var)\n",
    "        loss = rec + KLD\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # print(scheduler.get_last_lr())\n",
    "\n",
    "        if batch_idx % 25 == 0:\n",
    "            if vis: \n",
    "                visualize_latent_space(model, (rec, KLD), experiment_name, test_loader)\n",
    "                \n",
    "            save_recon(x_recon, experiment_name)\n",
    "            print(\"Epoch {:3} Iteration {:3}: recon: {:8.4f}, kld: {:8.4f}, std3: {:2.4f}\".format(epoch, batch_idx, rec.item(), KLD.item(), std3.item()))\n",
    "\n",
    "    path =  rf\"models\\{experiment_name}\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    save_model_to = rf\"{path}\\vae_{epoch}.pth\"\n",
    "    torch.save(model.state_dict(), save_model_to)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "\n",
    "\n",
    "# build model\n",
    "vae = VAE(sample_x=example_data, hidden_dims=None, z_dim=embedding_size)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### sanity check\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# vae.to(device)\n",
    "# vae.train()\n",
    "\n",
    "# for batch_idx, (x, _) in enumerate(data['train']):\n",
    "#     x = x.to(device)\n",
    "#     out, mu, log_var = vae(x)\n",
    "#     print(out.shape)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember: acceptable recon loss is 0.45\n",
    "num_epochs = 200\n",
    "\n",
    "experiment_name = f\"10_syntensor_only\"\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(vae, vae_loss, data['train'], data['test'], optimizer, scheduler, epoch, experiment_name, embedding_size=embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae, r\"models\\vae_orbita.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound_vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
